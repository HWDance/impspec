{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5020f6ee-6b7b-48c7-b395-50301c45ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# change import path\n",
    "path = Path.cwd().parents[1]\n",
    "if str(path) not in sys.path:\n",
    "    sys.path.append(str(path))\n",
    "\n",
    "from src.BayesIMP import *\n",
    "from src.kernels import *\n",
    "from src.dgps import *\n",
    "from src.CBO import *\n",
    "\n",
    "# CBO prior kernel class\n",
    "class CBOPriorKernel:\n",
    "    def __init__(model,kernel_func):\n",
    "        model.kernel_func = kernel_func\n",
    "\n",
    "    def get_gram(model,X,Z):\n",
    "        return model.kernel_func(X,Z)\n",
    "\n",
    "seed = 19\n",
    "n = 100 \n",
    "n_int = 100\n",
    "two_datasets = True\n",
    "niter = 1000\n",
    "learn_rate = 0.1\n",
    "optimise_mu = False\n",
    "exact = True\n",
    "mc_samples = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\"\"\" Bayesimp configs \"\"\"\n",
    "default_nu = 1.0\n",
    "reg = 1e-1\n",
    "Kernel = GaussianKernel\n",
    "quantiles = torch.linspace(0,1,101)[:,None]\n",
    "\n",
    "\"\"\" CBO configs \"\"\"\n",
    "int_samples = 10**5\n",
    "n_iter = 20\n",
    "xi = 0.0\n",
    "update_hyperparameters = False\n",
    "noise_init = -10.0\n",
    "cbo_reg = 1e-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fc6869-1560-4c55-a602-1540b4ce4fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/ghome/live/danceh/Causal-GP/causal-KL-GP/src/BayesIMP.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.tensor(scale_V_init, requires_grad = True))\n",
      "/nfs/ghome/live/danceh/Causal-GP/causal-KL-GP/src/BayesIMP.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.noise_Y = torch.tensor(noise_Y_init, requires_grad = True).float()\n",
      "/nfs/ghome/live/danceh/Causal-GP/causal-KL-GP/src/BayesIMP.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.kernel_V.dist.scale = torch.tensor(measure_init*V[1].var()**0.5).requires_grad_(optimise_measure)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 P(Y|V) loss:  tensor(889.4224)\n",
      "iter 100 P(Y|V) loss:  tensor(165.9147)\n",
      "iter 200 P(Y|V) loss:  tensor(164.5506)\n",
      "iter 300 P(Y|V) loss:  tensor(164.1805)\n",
      "iter 400 P(Y|V) loss:  tensor(164.0024)\n",
      "iter 500 P(Y|V) loss:  tensor(163.8721)\n",
      "iter 600 P(Y|V) loss:  tensor(163.7448)\n",
      "iter 700 P(Y|V) loss:  tensor(163.6001)\n",
      "iter 800 P(Y|V) loss:  tensor(163.4211)\n",
      "iter 900 P(Y|V) loss:  tensor(163.1900)\n",
      "iter 0 P(V|A) loss:  tensor(-21002.3613)\n",
      "iter 100 P(V|A) loss:  tensor(-38754.6602)\n",
      "iter 200 P(V|A) loss:  tensor(-39047.5703)\n",
      "iter 300 P(V|A) loss:  tensor(-39221.3125)\n",
      "iter 400 P(V|A) loss:  tensor(-39305.3789)\n",
      "iter 500 P(V|A) loss:  tensor(-39352.6328)\n",
      "iter 600 P(V|A) loss:  tensor(-39475.2383)\n",
      "iter 700 P(V|A) loss:  tensor(-39532.)\n",
      "iter 800 P(V|A) loss:  tensor(-39572.1914)\n",
      "iter 900 P(V|A) loss:  tensor(-39603.2695)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Draw int data \"\"\"\n",
    "dostatin = torch.linspace(0,1,n_int)\n",
    "age, bmi, aspirin, statin, cancer, psa = STATIN_PSA(int_samples, \n",
    "                                                    seed = seed, \n",
    "                                                    gamma = False, \n",
    "                                                    interventional_data = True, \n",
    "                                                    dostatin = dostatin)\n",
    "psa,fvol, vol = PSA_VOL(psa = psa)  \n",
    "\n",
    "\"\"\" Draw training data\"\"\"\n",
    "age_, bmi_, aspirin_, statin_, cancer_, psa_ = STATIN_PSA(n, \n",
    "                                                          seed = seed, \n",
    "                                                          gamma = False, \n",
    "                                                          interventional_data = False, \n",
    "                                                          dostatin=[])\n",
    "if two_datasets:\n",
    "    age_2, bmi_2, aspirin_2, statin_2, cancer_2, psa_2 = STATIN_PSA(n, \n",
    "                                                      seed = seed+1, \n",
    "                                                      gamma = False, \n",
    "                                                      interventional_data = False, \n",
    "                                                      dostatin=[])\n",
    "    psa_2, fvol_2, vol_2 = PSA_VOL(psa = psa_2)\n",
    "    A = torch.column_stack((age_, bmi_, aspirin_, statin_))\n",
    "    V = [psa_.reshape(len(psa_),1),psa_2.reshape(len(psa_),1)]\n",
    "    Y = vol_2\n",
    "    \n",
    "else:\n",
    "    psa_, fvol_, vol_ = PSA_VOL(psa = psa_)\n",
    "    A = torch.column_stack((age_, bmi_, aspirin_, statin_))\n",
    "    V = [psa_.reshape(len(psa_),1), psa_.reshape(len(psa_),1)]\n",
    "    Y = vol_\n",
    "\n",
    "\"\"\" Initialise model \"\"\"\n",
    "model = BayesIMP(Kernel_A = Kernel, \n",
    "               Kernel_V = Kernel, \n",
    "               Kernel_Z = [],\n",
    "               dim_A = A.size()[1], \n",
    "               dim_V = V[1].size()[1], \n",
    "               samples = 10**5,\n",
    "               exact = exact,\n",
    "               scale_V_init = Y.var()**0.5/2,\n",
    "               noise_Y_init = torch.log(Y.var()/4)\n",
    "              )\n",
    "\n",
    "\"\"\" Train model \"\"\"\n",
    "model.train(Y,A,V,niter,learn_rate, optimise_measure = optimise_mu, mc_samples = mc_samples, reg = reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cb998e-aefd-4fe3-b005-acaa30abfdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get posterior funcs and CBO prior kernel \"\"\"\n",
    "def mean(X):\n",
    "    doA = X.reshape(len(X),1)\n",
    "    return model.post_mean(Y,A,V,doA,\n",
    "                           reg = reg, \n",
    "                           average_doA = True, \n",
    "                           intervention_indices = [3]) \n",
    "\n",
    "def cov(X, Z, diag = False):\n",
    "    doA = X.reshape(len(X),1)\n",
    "    doA2 = Z.reshape(len(Z),1)\n",
    "    return model.post_var(Y,A,V,doA,doA2,\n",
    "                          reg = reg,\n",
    "                          average_doA = True, \n",
    "                          intervention_indices = [3], \n",
    "                          diag = diag)\n",
    "\n",
    "cbo_kernel = CBOPriorKernel(cov)\n",
    "\n",
    "# Define a grid of intervention points and precompute E[Y|do(x)]\n",
    "doX = dostatin[:,None]\n",
    "EYdoX = fvol.reshape(n_int,int_samples).mean(1)[:,None]\n",
    "\n",
    "# Random search for first intervention point\n",
    "torch.manual_seed(seed)\n",
    "start = torch.randint(0,99,(1,))[0]\n",
    "doXtrain, EYdoXtrain = doX[start].reshape(1,1), EYdoX[start].reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ab3c53-8c06-431b-91c4-4aae4cbdaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run CBO \"\"\"\n",
    "# Run CBO iters\n",
    "doXeval, EYdoXeval = causal_bayesian_optimization(X_train = doXtrain, \n",
    "                                                    y_train = EYdoXtrain, \n",
    "                                                    kernel = cbo_kernel, \n",
    "                                                    mean = mean,\n",
    "                                                    X_test = doX, \n",
    "                                                    Y_test = EYdoX, \n",
    "                                                    n_iter = n_iter, \n",
    "                                                    update_hyperparameters = update_hyperparameters,\n",
    "                                                    xi = xi, \n",
    "                                                    print_ = False, \n",
    "                                                    minimise = True,\n",
    "                                                    noise_init = noise_init,\n",
    "                                                    reg = cbo_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c33446-6faf-4bef-936f-4c31ceeff698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = doXtrain\n",
    "y_train = EYdoXtrain \n",
    "kernel = cbo_kernel\n",
    "mean = mean\n",
    "X_test = doX\n",
    "Y_test = EYdoX\n",
    "n_iter = n_iter\n",
    "update_hyperparameters = update_hyperparameters\n",
    "xi = xi\n",
    "print_ = False\n",
    "minimise = True\n",
    "noise_init = noise_init\n",
    "reg = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7012f2ed-8e4b-4b4d-96c7-f6c3a61f3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train is None or y_train is None:\n",
    "    X_train = torch.empty((0, X_test.shape[1]))  # Initialize empty tensor\n",
    "    y_train = torch.empty((0, 1))  # Initialize empty tensor\n",
    "\n",
    "# Initialize Gaussian Process model with the initial data and kernel\n",
    "gp = GaussianProcess(X_train=X_train, y_train=y_train, kernel=kernel, noise_init = noise_init, mean = mean, nugget = reg)\n",
    "\n",
    "# Initialize the maximum observed value\n",
    "y_best = torch.max(y_train) if len(y_train) > 0 else 0\n",
    "x_best = 0.5\n",
    "i = 0\n",
    "while x_best < 1.0 and i < n_iter:\n",
    "    # Get the GP predictions for the test grid\n",
    "    mu_s, cov_s = gp(X_test)\n",
    "    sigma_s = torch.sqrt(torch.diag(cov_s).abs())\n",
    "\n",
    "    # Calculate the Expected Improvement\n",
    "    ei = expected_improvement(mu_s[:,0], sigma_s, y_best, xi = xi, minimise = minimise)\n",
    "\n",
    "    # Find the next best point\n",
    "    next_index = torch.argmax(ei)\n",
    "    next_x = X_test[next_index]\n",
    "    next_y = Y_test[next_index]\n",
    "\n",
    "    # Update the training data with the new point\n",
    "    X_train = torch.cat((X_train, next_x.unsqueeze(0)), dim=0)\n",
    "    y_train = torch.cat((y_train, next_y.unsqueeze(0)), dim=0)\n",
    "\n",
    "    # Update GP model with new data\n",
    "    gp.X_train = X_train\n",
    "    gp.y_train = y_train\n",
    "\n",
    "    # Perform hyperparameter optimization if required\n",
    "    if update_hyperparameters and (i + 1) % update_interval == 0:\n",
    "        gp.optimize_hyperparameters(num_steps=hyperparam_steps, lr=lr, print_ = print_)\n",
    "\n",
    "    # Update the best observed value\n",
    "    if minimise:\n",
    "        y_best = torch.min(y_train)\n",
    "        x_best = X_train[torch.argmin(y_train)]\n",
    "    else:\n",
    "        y_best = torch.max(y_train)\n",
    "        x_best = X_train[torch.argmax(y_train)]\n",
    "        \n",
    "    if print_:\n",
    "        print(f\"Iteration {i+1}: X = {X_train.min()}, Y = {y_train.min()}\")\n",
    "\n",
    "    i += 1\n",
    "if i < n_iter:\n",
    "    X_train = torch.cat((X_train,x_best*torch.ones((n_iter - i,1))), dim = 0)\n",
    "    y_train = torch.cat((y_train,y_best*torch.ones((n_iter - i,1))), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e072634-11f9-4857-a3e8-439c201b4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "doA = X_test\n",
    "doA2 = []\n",
    "average_doA = True\n",
    "intervention_indices = [3]\n",
    "diag = False\n",
    "reg = 1e-2\n",
    "\n",
    "\n",
    "\n",
    "if not model.exact:\n",
    "    model.kernel_V.samples = samples\n",
    "\n",
    "# Getting second set of doA if computing covariance\n",
    "if doA2 ==[]:\n",
    "    doA2 = doA\n",
    "    \n",
    "# Dimensions\n",
    "n = len(Y)\n",
    "n0, N, M, D = len(A), len(doA), len(doA2), A.shape[1]\n",
    "Y = Y.reshape(n, 1)\n",
    "\n",
    "# Expand doA with replacement using the selected columns from A\n",
    "if average_doA:\n",
    "    average_indices = [j for j in range(D) if j not in intervention_indices]\n",
    "    expanded_doA = model.expand_doA(doA, A, intervention_indices)  # Shape (n0*N, D)\n",
    "    expanded_doA2 = model.expand_doA(doA2, A, intervention_indices)  # Shape (n0*M, D)\n",
    "else:\n",
    "    assert doA.shape[1] == A.shape[1]\n",
    "    assert doA2.shape[1] == A.shape[1]\n",
    "    expanded_doA = doA  # No expansion if not averaging, expanded_doA: (N, D)\n",
    "    expanded_doA2 = doA2  # No expansion if not averaging, expanded_doA: (M, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed03dc5-c866-446b-9865-cf551a11adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vall = torch.row_stack((V[0], V[1]))\n",
    "n1, n0 = len(Y), len(A)\n",
    "\n",
    "# Getting kernel matrices\n",
    "R_v1v0, R_v0v0, R_v1v1, R_vv1, R_vv0, R_vv = (\n",
    "    model.kernel_V.get_gram(V[1], V[0]),\n",
    "    model.kernel_V.get_gram(V[0], V[0]),\n",
    "    model.kernel_V.get_gram(V[1], V[1]),\n",
    "    model.kernel_V.get_gram(Vall, V[1]),\n",
    "    model.kernel_V.get_gram(Vall, V[0]),\n",
    "    model.kernel_V.get_gram(Vall, Vall)\n",
    ")\n",
    "K_v0v0, K_v1v1, K_vv1, Kvv0, K_vv = (\n",
    "    model.kernel_V.get_gram_base(V[0], V[0]),\n",
    "    model.kernel_V.get_gram_base(V[1], V[1]),\n",
    "    model.kernel_V.get_gram_base(Vall, V[1]),\n",
    "    model.kernel_V.get_gram_base(Vall, V[0]),\n",
    "    model.kernel_V.get_gram_base(Vall, Vall)\n",
    ")\n",
    "K_aa, k_atest,k_atest2 = (\n",
    "    model.kernel_A.get_gram(A, A),\n",
    "    model.kernel_A.get_gram(expanded_doA, A),\n",
    "    model.kernel_A.get_gram(expanded_doA2, A)\n",
    ")\n",
    "R_v1 = R_v1v1 + (model.noise_Y.exp() + reg) * torch.eye(n1)\n",
    "K_v1 = K_v1v1 + (model.noise_Y.exp() + reg) * torch.eye(n1)\n",
    "K_a = K_aa + (model.noise_feat.exp() + reg) * torch.eye(n0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7d500f-69ed-4cb4-bb67-75434716d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging out selected indices\n",
    "if average_doA:\n",
    "    k_atest = k_atest.reshape(n0,N,n0).mean(0) # (N,n0)\n",
    "    k_atest2 = k_atest2.reshape(n0,M,n0).mean(0) # (M,n0)    \n",
    "\n",
    "# Computing matrix vector products\n",
    "Theta1 = torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv0) @ torch.linalg.solve(R_v0v0+torch.eye(n0)*reg, K_v0v0)  # (n1+n0, n1+n0)\n",
    "Theta4 = torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv1) @ torch.linalg.solve(K_v1, Y)  # (n1+n0, 1)\n",
    "Theta2a = Theta4.T @ R_vv @ Theta4  # (1, 1)\n",
    "Theta2b = Theta4.T @ R_vv0 @ torch.linalg.solve(R_v0v0+torch.eye(n0)*reg, R_vv0.T) @ Theta4  # (1, 1)\n",
    "Theta3a = torch.trace(torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv) @ torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv - R_vv1 @ torch.linalg.solve(R_v1, R_vv1.T)))  # scalar\n",
    "Theta3b = torch.trace(torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv0) \n",
    "                      @ torch.linalg.solve(R_v0v0+torch.eye(n0)*reg, R_vv0.T) \n",
    "                      @ torch.linalg.solve(K_vv+torch.eye(n1+n0)*reg, R_vv - R_vv1 @ torch.linalg.solve(R_v1, R_vv1.T)))  # scalar \n",
    "E_a = torch.linalg.solve(K_a, k_atest.T)  # (n0, N)\n",
    "E_a2 = torch.linalg.solve(K_a, k_atest2.T)  # (n0, M)\n",
    "G_aa = E_a.T @ k_atest2.T\n",
    "\n",
    "# Get gram matrix on doA,doA2\n",
    "if average_doA: \n",
    "    \n",
    "    # If averaging, define separate kernels for averaging and intervention indices\n",
    "    kernel_Aavg = deepcopy(model.kernel_A)\n",
    "    kernel_Aavg.lengthscale = model.kernel_A.lengthscale[average_indices]\n",
    "    kernel_Aavg.scale = 1.0\n",
    "\n",
    "    kernel_doA = deepcopy(model.kernel_A)\n",
    "    kernel_doA.lengthscale = model.kernel_A.lengthscale[intervention_indices]\n",
    "    \n",
    "    # Construct average and interventional gram matrices\n",
    "    Aavg = A[:,average_indices].reshape(n0,len(average_indices))\n",
    "    K_Aavg = kernel_Aavg.get_gram(Aavg,Aavg).mean()\n",
    "    K_doA = kernel_doA.get_gram(doA,doA2)\n",
    "\n",
    "    K_atestatest = K_Aavg*K_doA\n",
    "else:\n",
    "    # otherwise, just construct N x M gram matrix on doA \n",
    "    K_atestatest = model.kernel_A.get_gram(doA,doA2)            \n",
    "F_aa = K_atestatest  # (N,M)\n",
    "\n",
    "# Final computations\n",
    "V1 = E_a.T @ Theta1.T @ (R_vv - R_vv1 @ torch.linalg.solve(R_v1, R_vv1.T)) @ Theta1 @ E_a2  # (N,M)\n",
    "V2 = Theta2a * F_aa - Theta2b * G_aa  # (N,M)\n",
    "V3 = Theta3a * F_aa - Theta3b * G_aa  # (N,M)\n",
    "\n",
    "if not diag:\n",
    "    posterior_variance = V1 + V2 + V3 # (N,M)\n",
    "else:\n",
    "    posterior_variance = (V1 + V2 + V3).diag().reshape(N,1)  # (N,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
